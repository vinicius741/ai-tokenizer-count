---
phase: 03-cli-polish
plan: 04
type: execute
wave: 2
depends_on: ["03-03"]
files_modified:
  - src/output/summary.ts
  - src/cli/index.ts
autonomous: true

must_haves:
  truths:
    - "User sees summary statistics after batch completion"
    - "Summary shows total EPUBs processed, successful, failed"
    - "Summary shows total words and average words per EPUB"
    - "Summary shows total tokens per tokenizer"
    - "Summary shows timing information (total time, avg per file)"
  artifacts:
    - path: "src/output/summary.ts"
      provides: "Summary statistics aggregation and table formatting"
      min_lines: 100
      exports: ["SummaryStats", "calculateSummary", "displaySummary"]
    - path: "src/cli/index.ts"
      provides: "CLI integration with summary display"
      contains: "import.*from.*summary|displaySummary"
  key_links:
    - from: "src/cli/index.ts"
      to: "src/output/summary.ts"
      via: "import { calculateSummary, displaySummary }"
      pattern: "import.*summary"
    - from: "src/output/summary.ts"
      to: "cli-table3"
      via: "Table import and usage"
      pattern: "from 'cli-table3'"
    - from: "src/output/summary.ts"
      to: "ProcessingResult"
      via: "Aggregating results from processEpubsWithErrors"
      pattern: "successful|failed|tokenCounts"
---

<objective>
Implement summary statistics display with aggregated metrics (total EPUBs, words, tokens, timing) formatted in professional tables.

Purpose: Users need comprehensive summary statistics after batch processing to understand results at a glance: how many files processed, total word/token counts, failures, and timing information.

Output: Working summary statistics system with sectioned table blocks (overview, tokenizer stats, failures) and timing information.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-cli-polish/03-RESEARCH.md
@.planning/phases/03-cli-polish/03-CONTEXT.md

# Codebase context
@src/cli/index.ts
@src/errors/handler.ts
@src/output/table.ts
@.planning/phases/03-cli-polish/03-01-PLAN.md
@.planning/phases/03-cli-polish/03-03-PLAN.md
</context>

<tasks>

<task type="auto">
  <name>Create summary statistics module</name>
  <files>src/output/summary.ts</files>
  <action>
    Create src/output/summary.ts with summary statistics calculation and display:

    **Requirements from CONTEXT.md:**
    - Content: Total EPUBs, total words, total tokens (per tokenizer), failures, averages
    - Structure: Sectioned blocks (overview, tokenizer stats, failures)
    - Timing: Total time, avg time/EPUB, fastest/slowest file
    - Format: cli-table3 (already in dependencies)

    **Code structure:**
    ```typescript
    import Table from 'cli-table3';
    import type { EpubResult } from './table.js';
    import type { TokenizerResult } from '../tokenizers/types.js';

    /**
     * Summary statistics for batch processing
     */
    export interface SummaryStats {
      totalEpubs: number;
      successfulEpubs: number;
      failedEpubs: number;
      totalWords: number;
      avgWordsPerEpub: number;
      totalTokens: Map<string, number>; // tokenizer name -> total count
      avgTokensPerEpub: Map<string, number>; // tokenizer name -> average
      totalTimeMs: number;
      avgTimePerEpubMs: number;
      fastestEpub: { filename: string; timeMs: number } | null;
      slowestEpub: { filename: string; timeMs: number } | null;
      failures: Array<{ file: string; error: string }>;
    }

    /**
     * Calculate summary statistics from processing results
     */
    export function calculateSummary(
      successful: EpubResult[],
      failed: Array<{ file: string; error: string; suggestion?: string }>,
      tokenCounts: Map<string, TokenizerResult[]>,
      startTime: number
    ): SummaryStats {
      const endTime = Date.now();
      const totalTimeMs = endTime - startTime;

      // Count totals
      const totalEpubs = successful.length + failed.length;
      const successfulEpubs = successful.length;
      const failedEpubs = failed.length;

      // Word counts
      const totalWords = successful.reduce((sum, r) => sum + r.wordCount, 0);
      const avgWordsPerEpub = successfulEpubs > 0 ? Math.round(totalWords / successfulEpubs) : 0;

      // Token counts (aggregate per tokenizer)
      const totalTokens = new Map<string, number>();
      const avgTokensPerEpub = new Map<string, number>();

      // Get all tokenizer names from first result (or all results)
      const tokenizerNames = new Set<string>();
      for (const counts of tokenCounts.values()) {
        for (const t of counts) {
          tokenizerNames.add(t.name);
        }
      }

      // Aggregate totals per tokenizer
      for (const name of tokenizerNames) {
        let sum = 0;
        let count = 0;

        for (const counts of tokenCounts.values()) {
          const result = counts.find((t) => t.name === name);
          if (result && result.count > 0) { // Skip -1 (errors) and 0 (empty)
            sum += result.count;
            count++;
          }
        }

        totalTokens.set(name, sum);
        avgTokensPerEpub.set(name, count > 0 ? Math.round(sum / count) : 0);
      }

      // Timing
      const avgTimePerEpubMs = totalEpubs > 0 ? Math.round(totalTimeMs / totalEpubs) : 0;

      // Fastest/slowest (placeholder - requires per-file timing)
      const fastestEpub = null;
      const slowestEpub = null;

      // Failures
      const failures = failed.map((f) => ({ file: f.file, error: f.error }));

      return {
        totalEpubs,
        successfulEpubs,
        failedEpubs,
        totalWords,
        avgWordsPerEpub,
        totalTokens,
        avgTokensPerEpub,
        totalTimeMs,
        avgTimePerEpubMs,
        fastestEpub,
        slowestEpub,
        failures,
      };
    }

    /**
     * Display summary statistics in formatted tables
     */
    export function displaySummary(stats: SummaryStats): void {
      console.log('\n' + '='.repeat(60));
      console.log('SUMMARY STATISTICS');
      console.log('='.repeat(60));

      // Overview table
      const overviewTable = new Table({
        head: ['Metric', 'Value'],
        colWidths: [30, 28],
        wordWrap: true,
        style: {
          head: ['cyan', 'bold'],
          border: ['grey'],
        },
      });

      overviewTable.push(
        ['Total EPUBs processed', stats.totalEpubs.toString()],
        ['Successful', stats.successfulEpubs.toString()],
        ['Failed', stats.failedEpubs.toString()],
        ['Total words', stats.totalWords.toLocaleString()],
        ['Average words/EPUB', stats.avgWordsPerEpub.toLocaleString()],
        ['Total time', formatDuration(stats.totalTimeMs)],
        ['Average time/EPUB', formatDuration(stats.avgTimePerEpubMs)],
      );

      console.log('\nüìä Overview');
      console.log(overviewTable.toString());

      // Tokenizer statistics table
      if (stats.totalTokens.size > 0) {
        const tokenTable = new Table({
          head: ['Tokenizer', 'Total Tokens', 'Avg Tokens/EPUB'],
          colWidths: [20, 20, 20],
          wordWrap: true,
          style: {
            head: ['cyan', 'bold'],
            border: ['grey'],
          },
        });

        for (const [name, total] of stats.totalTokens.entries()) {
          const avg = stats.avgTokensPerEpub.get(name) || 0;
          tokenTable.push([name, total.toLocaleString(), avg.toLocaleString()]);
        }

        console.log('\nüî¢ Tokenizer Statistics');
        console.log(tokenTable.toString());
      }

      // Failures table
      if (stats.failures.length > 0) {
        const failureTable = new Table({
          head: ['File', 'Error'],
          colWidths: [40, 40],
          wordWrap: true,
          style: {
            head: ['red', 'bold'],
            border: ['grey'],
          },
        });

        for (const failure of stats.failures) {
          const filename = failure.file.split('/').pop() || failure.file;
          failureTable.push([filename, failure.error]);
        }

        console.log('\n‚ùå Failures');
        console.log(failureTable.toString());
      }

      console.log('');
    }

    /**
     * Format milliseconds as human-readable duration
     */
    function formatDuration(ms: number): string {
      if (ms < 1000) {
        return `${ms}ms`;
      } else if (ms < 60000) {
        return `${(ms / 1000).toFixed(1)}s`;
      } else {
        const mins = Math.floor(ms / 60000);
        const secs = ((ms % 60000) / 1000).toFixed(1);
        return `${mins}m ${secs}s`;
      }
    }
    ```

    **Design decisions:**
    - Sectioned blocks: Overview, Tokenizer Stats, Failures (as recommended in RESEARCH.md)
    - Use emojis for section headers (üìä, üî¢, ‚ùå) for visual clarity
    - Format numbers with locale strings (1,234,567) for readability
    - Format durations (ms, s, m) for human-readability
    - Tokenizer table only shows if tokenizers used (totalTokens.size > 0)
    - Failures table only shows if failures exist (failures.length > 0)
    - Red header for failures table (error indication)
    - Skip -1 token counts (errors) when calculating averages
  </action>
  <verify>Check that:
  - src/output/summary.ts exists
  - Exports SummaryStats interface
  - Exports calculateSummary function
  - Exports displaySummary function
  - calculateSummary aggregates: totals, averages, token counts, timing
  - displaySummary shows: overview, tokenizer stats, failures
  - Uses cli-table3 for formatting
  - Formats numbers and durations for readability
  </verify>
  <done>Summary module with statistics calculation and display</done>
</task>

<task type="auto">
  <name>Integrate summary display into CLI</name>
  <files>src/cli/index.ts</files>
  <action>
    Modify src/cli/index.ts to display summary after processing:

    **Changes:**

    1. Add imports:
    ```typescript
    import { calculateSummary, displaySummary } from '../output/summary.js';
    ```

    2. Update processEpubs function to capture timing and display summary:
    ```typescript
    async function processEpubs(inputPaths: string[], options: any): Promise<void> {
      // ... existing file discovery code ...

      const outputDir = options.output || './results';
      const tokenizers = options.tokenizers || ['gpt4'];
      const maxMb = options.maxMb || 500;

      // Capture start time for summary statistics
      const startTime = Date.now();

      // Create MultiBar for progress display
      const multibar = createProgressBars();

      let result;

      // ... existing processing code (parallel or sequential) ...

      // Stop all progress bars
      stopAll(multibar);

      // Display successful EPUBs in table (existing)
      displayResults(result.successful, { verbose: options.verbose });

      // Generate and write results files (existing)
      const mdPath = await writeResultsFile(result, { outputDir });
      const jsonPath = await writeJsonFile(result, { outputDir }, result.tokenCounts);

      // Display results file paths (existing)
      console.log(`\nResults saved to:`);
      console.log(`- ${mdPath}`);
      console.log(`- ${jsonPath}`);

      // NEW: Display summary statistics
      const summary = calculateSummary(
        result.successful,
        result.failed,
        result.tokenCounts,
        startTime
      );
      displaySummary(summary);
    }
    ```

    **Important:** Replace the existing simple summary (lines 80-92 in current code) with the comprehensive summary from summary.ts. Remove:
    ```typescript
    // REMOVE THIS CODE:
    console.log(`\nSummary:`);
    console.log(`- Total EPUBs: ${result.total}`);
    console.log(`- Successful: ${result.successful.length}`);
    console.log(`- Failed: ${result.failed.length}`);
    ```

    **Timing placement:**
    - Start time: Before any processing (after file discovery)
    - End time: After all processing completes (before summary display)
    - This captures processing time only, not file discovery time
  </action>
  <verify>Check that:
  - Imports calculateSummary and displaySummary
  - Captures startTime before processing
  - Calls calculateSummary after processing
  - Calls displaySummary to show formatted tables
  - Removes old simple summary (console.log statements)
  - Summary appears after results file paths
  </verify>
  <done>Summary statistics displayed after batch completion</done>
</task>

</tasks>

<verification>
1. Test summary display with multiple EPUB files
2. Verify all statistics are calculated correctly
3. Verify table formatting and sectioned blocks
4. Test with failures (should show failures table)
5. Test timing information accuracy

Test commands:
```bash
# Test with multiple files
epub-counter ./epubs/

# Test with failures (corrupted EPUB in folder)
cp invalid.epub ./epubs/
epub-counter ./epubs/

# Test with multiple tokenizers
epub-counter ./epubs/ --tokenizers gpt4,claude

# Test parallel processing timing
time epub-counter ./epubs/ --jobs 4
```

Expected output:
- Overview table: Total EPUBs, successful, failed, total words, averages, timing
- Tokenizer table: Total tokens and average per EPUB for each tokenizer
- Failures table: List of failed files with error messages (if any)
- Formatted numbers: 1,234,567 (with locale)
- Formatted durations: 1m 23.4s or 5.2s or 123ms
</verification>

<success_criteria>
- Summary statistics displayed after batch completion
- Overview table shows totals and averages
- Tokenizer table shows total and average tokens per tokenizer
- Failures table shows failed files (if any)
- Timing information shows total time and average per file
- Tables properly formatted with cli-table3
- Numbers formatted for readability (locale strings, durations)
</success_criteria>

<output>
After completion, create `.planning/phases/03-cli-polish/03-04-SUMMARY.md`
</output>
