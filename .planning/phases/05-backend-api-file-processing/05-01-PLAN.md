---
phase: 05-backend-api-file-processing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - server/src/server.ts
  - server/src/routes/list-models.ts
  - packages/shared/src/types.ts
autonomous: true
user_setup: []

must_haves:
  truths:
    - User can GET /api/list-models and receive JSON array of available tokenizers
    - Response includes GPT-4, Claude, and popular Hugging Face models
    - Each model entry has id, name, description, and async flag
    - Response format matches shared types (TokenizerInfo[])
  artifacts:
    - path: "server/src/routes/list-models.ts"
      provides: "GET /api/list-models endpoint"
      exports: ["listModelsHandler"]
    - path: "packages/shared/src/types.ts"
      provides: "TokenizerInfo type definition"
      contains: "interface TokenizerInfo"
  key_links:
    - from: "server/src/server.ts"
      to: "server/src/routes/list-models.ts"
      via: "fastify.register() or route handler import"
      pattern: "list-models|listModels"
    - from: "server/src/routes/list-models.ts"
      to: "src/tokenizers/hf-models.ts"
      via: "import and reuse existing HuggingFace models list"
      pattern: "hf-models|HUGGING_FACE_MODELS"
---

<objective>
Implement GET /api/list-models endpoint that returns all available tokenizers (GPT-4, Claude, and popular Hugging Face models)

Purpose: Provide frontend with tokenizer options for user selection
Output: Working API endpoint returning TokenizerInfo[] JSON array
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/05-backend-api-file-processing/05-CONTEXT.md
@server/src/server.ts
@packages/shared/src/types.ts
@src/tokenizers/hf-models.ts
@src/cli/commands/list-models.ts
@.planning/phases/04-foundation-project-setup/04-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verify TokenizerInfo type exists in shared package</name>
  <files>packages/shared/src/types.ts</files>
  <action>
    Verify TokenizerInfo already exists in packages/shared/src/types.ts.
    The type should have:
    - id: TokenizerType
    - name: string
    - description?: string
    - async: boolean

    If missing, add the TokenizerInfo interface. If present, no changes needed.
  </action>
  <verify>grep -q "interface TokenizerInfo" packages/shared/src/types.ts && echo "TokenizerInfo type exists"</verify>
  <done>TokenizerInfo type is exported from @epub-counter/shared with required fields (id, name, description?, async)</done>
</task>

<task type="auto">
  <name>Task 2: Create /api/list-models endpoint</name>
  <files>server/src/routes/list-models.ts</files>
  <action>
    Create server/src/routes/list-models.ts with:

    1. Import TokenizerInfo from @epub-counter/shared
    2. Reuse HUGGING_FACE_MODELS from src/tokenizers/hf-models.ts (convert path to workspace-relative import)
    3. Create listModels array with:
       - gpt4: { id: 'gpt4', name: 'GPT-4', description: 'OpenAI GPT-4 tokenizer (cl100k_base)', async: false }
       - claude: { id: 'claude', name: 'Claude', description: 'Anthropic Claude tokenizer', async: false }
       - All HF models from HUGGING_FACE_MODELS, prefixed with 'hf:', async: true
    4. Export async function listModelsHandler(fastify) that registers GET /api/list-models
    5. Return the list as JSON

    Reference src/cli/commands/list-models.ts for existing HuggingFace models list.

    Use workspace import for hf-models: Add "../../../src/tokenizers/hf-models" to server/tsconfig.json paths or use relative path.
  </action>
  <verify>curl -s http://localhost:8787/api/list-models | jq '.[0].id' | grep -q "gpt4"</verify>
  <done>GET /api/list-models returns TokenizerInfo[] JSON array with gpt4, claude, and HF models</done>
</task>

<task type="auto">
  <name>Task 3: Register list-models route in server</name>
  <files>server/src/server.ts</files>
  <action>
    Import and register the list-models route in server/src/server.ts:

    ```typescript
    import { listModelsHandler } from './routes/list-models.js'

    // After CORS registration, before start():
    await fastify.register(listModelsHandler)
    ```

    Ensure the route is registered after CORS setup.
  </action>
  <verify>curl -s http://localhost:8787/api/list-models | jq 'length' | grep -q '[1-9]'</verify>
  <done>/api/list-models endpoint is accessible and returns tokenizers list</done>
</task>

</tasks>

<verification>
1. Start server: npm run dev:server (or cd server && npm run dev)
2. Test endpoint: curl http://localhost:8787/api/list-models
3. Verify response has id, name, description, async fields
4. Verify gpt4 and claude are first entries
5. Verify HF models have 'hf:' prefix and async: true
6. Verify response is valid JSON array
</verification>

<success_criteria>
- GET /api/list-models returns 200 status
- Response contains at least gpt4, claude, and 5+ HF models
- Each tokenizer has required fields: id, name, description?, async
- HF model IDs start with 'hf:'
- Response format matches TokenizerInfo[] type
</success_criteria>

<output>
After completion, create `.planning/phases/05-backend-api-file-processing/05-01-SUMMARY.md`
</output>
