---
phase: 02-tokenization-engine
plan: 02
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - src/tokenizers/gpt.ts
  - src/tokenizers/claude.ts
  - src/tokenizers/huggingface.ts
autonomous: true
user_setup: []

must_haves:
  truths:
    - "GPT-4 tokenizer uses js-tiktoken encoding_for_model('gpt-4')"
    - "Claude tokenizer uses @anthropic-ai/tokenizer countTokens()"
    - "Hugging Face tokenizer uses AutoTokenizer.from_pretrained()"
    - "All tokenizers implement the Tokenizer interface"
    - "Tokenizers are initialized lazily (on first use)"
  artifacts:
    - path: "src/tokenizers/gpt.ts"
      provides: "GPT4Tokenizer class implementing Tokenizer"
      exports: ["GPT4Tokenizer"]
      contains: "encoding_for_model"
    - path: "src/tokenizers/claude.ts"
      provides: "ClaudeTokenizer class implementing Tokenizer"
      exports: ["ClaudeTokenizer"]
      contains: "countTokens"
    - path: "src/tokenizers/huggingface.ts"
      provides: "HFTokenizer class implementing Tokenizer"
      exports: ["HFTokenizer"]
      contains: "AutoTokenizer"
  key_links:
    - from: "src/tokenizers/gpt.ts"
      to: "js-tiktoken"
      via: "import { encoding_for_model } from 'js-tiktoken'"
      pattern: "encoding_for_model.*gpt-4"
    - from: "src/tokenizers/claude.ts"
      to: "@anthropic-ai/tokenizer"
      via: "import { countTokens } from '@anthropic-ai/tokenizer'"
      pattern: "countTokens"
    - from: "src/tokenizers/huggingface.ts"
      to: "@huggingface/transformers"
      via: "import { AutoTokenizer } from '@huggingface/transformers'"
      pattern: "AutoTokenizer.*from_pretrained"
---

<objective>
Implement the three core tokenizer classes for GPT-4, Claude, and Hugging Face models.

Purpose: Create concrete implementations of the Tokenizer interface for the three supported tokenizer types. Each implementation wraps its respective library (js-tiktoken for GPT-4, @anthropic-ai/tokenizer for Claude, @huggingface/transformers for HF models) and provides a consistent countTokens() API.

Output: Three tokenizer implementation files (gpt.ts, claude.ts, huggingface.ts) with lazy initialization and proper resource management.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plan
@.planning/phases/02-tokenization-engine/02-01-PLAN.md

# Codebase context
@src/tokenizers/types.ts
@src/output/json.ts
@src/errors/handler.ts

# Research findings (CRITICAL for implementation patterns)
@.planning/phases/02-tokenization-engine/02-RESEARCH.md
@.planning/phases/02-tokenization-engine/02-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Implement GPT-4 tokenizer</name>
  <files>src/tokenizers/gpt.ts</files>
  <action>
    Create src/tokenizers/gpt.ts with GPT4Tokenizer class:

    **Implementation Pattern (from RESEARCH.md):**
    ```typescript
    import { encoding_for_model, Tiktoken } from 'js-tiktoken';
    import type { Tokenizer, TokenizerResult } from './types.js';

    export class GPT4Tokenizer implements Tokenizer {
      name = 'gpt4';
      private encoder: Tiktoken | null = null;

      constructor() {
        // Lazy initialization - don't create encoder yet
      }

      private getEncoder(): Tiktoken {
        if (!this.encoder) {
          // Use encoding_for_model('gpt-4') for future-proofing
          this.encoder = encoding_for_model('gpt-4');
        }
        return this.encoder;
      }

      countTokens(text: string): number {
        const enc = this.getEncoder();
        const tokens = enc.encode(text);
        return tokens.length;
      }

      // Cleanup method to free WASM memory
      dispose(): void {
        if (this.encoder) {
          this.encoder.free();
          this.encoder = null;
        }
      }
    }
    ```

    **Key considerations from RESEARCH.md:**
    - Use encoding_for_model('gpt-4') not cl100k_base (future-proofs for new encodings)
    - Lazy initialization: don't create encoder in constructor
    - Call .free() to release WASM memory when done
    - Per CONTEXT.md: Use "gpt4" as the name, not "cl100k_base"

    Follow existing codebase patterns:
    - ES6 modules (import/export with .js extensions)
    - JSDoc comments
    - TypeScript strict mode
  </action>
  <verify>grep -E "(class GPT4Tokenizer|encoding_for_model|countTokens)" src/tokenizers/gpt.ts returns expected implementations</verify>
  <done>GPT4Tokenizer class exists with lazy initialization, uses encoding_for_model('gpt-4'), implements countTokens()</done>
</task>

<task type="auto">
  <name>Implement Claude tokenizer</name>
  <files>src/tokenizers/claude.ts</files>
  <action>
    Create src/tokenizers/claude.ts with ClaudeTokenizer class:

    **Implementation Pattern (from RESEARCH.md):**
    ```typescript
    import { countTokens } from '@anthropic-ai/tokenizer';
    import type { Tokenizer } from './types.js';

    export class ClaudeTokenizer implements Tokenizer {
      name = 'claude';

      countTokens(text: string): number {
        // Direct call to official Anthropic tokenizer
        return countTokens(text);
      }
    }
    ```

    **CRITICAL: Add warning about Claude 3+ limitation**
    From RESEARCH.md Pitfall 1: The official package is documented as inaccurate for Claude 3+
    Add JSDoc comment:
    ```typescript
    /**
     * Claude tokenizer using @anthropic-ai/tokenizer
     *
     * WARNING: This tokenizer is only accurate for Claude 1 and Claude 2.
     * Claude 3+ uses a new tokenizer that is not publicly documented.
     * This implementation provides an approximation but counts are not guaranteed to be accurate.
     *
     * For accurate Claude 3+ token counts, use the Anthropic API response usage field.
     */
    ```

    **Key considerations from RESEARCH.md:**
    - Use official @anthropic-ai/tokenizer package
    - Simple countTokens(text) function call
    - Document the Claude 3+ inaccuracy prominently
    - Per CONTEXT.md: Use "claude" as the name

    Follow existing codebase patterns:
    - ES6 modules
    - JSDoc comments with warning
  </action>
  <verify>grep -E "(class ClaudeTokenizer|countTokens|Claude 3)" src/tokenizers/claude.ts returns expected implementation and warning</verify>
  <done>ClaudeTokenizer class exists, uses @anthropic-ai/tokenizer countTokens(), includes Claude 3+ warning in JSDoc</done>
</task>

<task type="auto">
  <name>Implement Hugging Face tokenizer</name>
  <files>src/tokenizers/huggingface.ts</files>
  <action>
    Create src/tokenizers/huggingface.ts with HFTokenizer class:

    **Implementation Pattern (from RESEARCH.md):**
    ```typescript
    import { AutoTokenizer } from '@huggingface/transformers';
    import type { Tokenizer } from './types.js';

    export class HFTokenizer implements Tokenizer {
      name: string;
      private tokenizer: any = null;
      private initialized = false;

      constructor(modelName: string) {
        this.name = modelName;
      }

      private async initialize(): Promise<void> {
        if (this.initialized) return;

        // Load tokenizer from Hugging Face Hub
        this.tokenizer = await AutoTokenizer.from_pretrained(this.name);
        this.initialized = true;
      }

      async countTokens(text: string): Promise<number> {
        await this.initialize();

        // Tokenize text and get token IDs
        const { input_ids } = await this.tokenizer.encode(text);
        return input_ids.length;
      }
    }
    ```

    **Key considerations from RESEARCH.md:**
    - Use AutoTokenizer.from_pretrained(modelName)
    - Lazy initialization (download model on first use)
    - Async countTokens() (HF tokenizer loading is async)
    - Per CONTEXT.md: Remote HF Hub models only (no local file paths)
    - Model name is stored in this.name (dynamic based on constructor)

    **Error handling:**
    - Wrap model loading in try/catch
    - Throw helpful error if model name is invalid
    - Suggest checking HF Hub for valid model names

    Follow existing codebase patterns:
    - ES6 modules
    - JSDoc comments
    - Async/await for tokenizer initialization
  </action>
  <verify>grep -E "(class HFTokenizer|AutoTokenizer|from_pretrained|async countTokens)" src/tokenizers/huggingface.ts returns expected implementation</verify>
  <done>HFTokenizer class exists with lazy initialization, uses AutoTokenizer.from_pretrained(), implements async countTokens()</done>
</task>

</tasks>

<verification>
After completion, verify:
- [ ] All three tokenizer files exist (gpt.ts, claude.ts, huggingface.ts)
- [ ] TypeScript compilation succeeds (npm run build)
- [ ] GPT4Tokenizer uses encoding_for_model('gpt-4')
- [ ] ClaudeTokenizer includes Claude 3+ warning in JSDoc
- [ ] HFTokenizer has async countTokens() method
- [ ] All classes implement Tokenizer interface
</verification>

<success_criteria>
1. All three tokenizer classes compile without errors
2. Each class can be imported and instantiated
3. GPT4Tokenizer and ClaudeTokenizer have sync countTokens(), HFTokenizer has async countTokens()
4. JSDoc comments document usage and limitations
</success_criteria>

<output>
After completion, create `.planning/phases/02-tokenization-engine/02-02-SUMMARY.md`
</output>
