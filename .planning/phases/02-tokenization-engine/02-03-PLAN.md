---
phase: 02-tokenization-engine
plan: 03
type: execute
wave: 3
depends_on: [02-01]
files_modified:
  - src/cli/index.ts
  - src/output/json.ts
autonomous: true
user_setup: []

must_haves:
  truths:
    - "CLI accepts --tokenizers flag with comma-separated list"
    - "Default tokenizer is 'gpt4'"
    - "CLI parses 'gpt4,claude,hf:bert-base-uncased' into array"
    - "JSON output includes token_counts array with {name, count} objects"
    - "JSON output includes schema_version field set to '1.0'"
  artifacts:
    - path: "src/cli/index.ts"
      provides: "CLI argument parsing for --tokenizers"
      contains: "--tokenizers"
    - path: "src/output/json.ts"
      provides: "Extended JSON output with token_counts"
      contains: "token_counts"
      contains: "schema_version"
  key_links:
    - from: "src/cli/index.ts"
      to: "src/tokenizers/types.ts"
      via: "TokenizerFactory will use parsed tokenizer names"
      pattern: "tokenizers.*split.*,"
    - from: "src/output/json.ts"
      to: "src/tokenizers/types.ts"
      via: "EpubJsonResult extended with token_counts field"
      pattern: "token_counts.*TokenizerResult"
---

<objective>
Integrate tokenization into the CLI and extend JSON output to include token counts.

Purpose: Enable users to select tokenizers via CLI arguments and receive token counts in JSON output. The --tokenizers flag accepts comma-separated presets (gpt4, claude) and Hugging Face models (hf:model-name). JSON output is extended with a token_counts array and schema_version field.

Output: Updated CLI with --tokenizers argument, and extended JSON output including token_counts array and schema_version.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plans
@.planning/phases/02-tokenization-engine/02-01-PLAN.md
@.planning/phases/02-tokenization-engine/02-02-PLAN.md

# Codebase context
@src/cli/index.ts
@src/output/json.ts
@src/tokenizers/types.ts

# Research findings
@.planning/phases/02-tokenization-engine/02-RESEARCH.md
@.planning/phases/02-tokenization-engine/02-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Add --tokenizers CLI argument</name>
  <files>src/cli/index.ts</files>
  <action>
    Extend src/cli/index.ts to add --tokenizers argument:

    **Add new CLI option:**
    ```typescript
    program
      // ... existing options ...
      .option('-t, --tokenizers <list>', 'Comma-separated list of tokenizers (e.g., gpt4,claude,hf:bert-base-uncased)', 'gpt4')
      .action((paths, options) => {
        // ... existing code ...

        // Parse tokenizers list
        const tokenizers = options.tokenizers.split(',').map((t: string) => t.trim());

        if (options.verbose) {
          console.log('Tokenizers:', tokenizers);
        }

        // Pass tokenizers to processEpubs
        await processEpubs(inputPaths, { ...options, tokenizers });
      });
    ```

    **Update processEpubs signature:**
    ```typescript
    async function processEpubs(inputPaths: string[], options: any): Promise<void> {
      // ... existing code ...

      // Extract tokenizers from options
      const tokenizers = options.tokenizers || ['gpt4'];

      // Pass to processEpubsWithErrors (will be wired in next plan)
      const result = await processEpubsWithErrors(
        allFiles,
        options.verbose,
        outputDir,
        tokenizers  // New parameter
      );
      // ... rest of function ...
    }
    ```

    **Tokenizer name format (from CONTEXT.md):**
    - Presets: "gpt4", "claude" (short names, not cl100k_base)
    - Hugging Face: "hf:model-name" (e.g., "hf:bert-base-uncased", "hf:gpt2")
    - Multiple: comma-separated (e.g., "gpt4,claude,hf:gpt2")

    **Default behavior:**
    - If --tokenizers not provided, default to ['gpt4']
    - Per CONTEXT.md decision: Claude's discretion on default (choosing 'gpt4' as most widely-adopted)

    Follow existing CLI patterns:
    - Use short flag (-t) and long flag (--tokenizers)
    - Default value in .option() call
    - Help text documents usage and examples
    - Verbose logging shows selected tokenizers
  </action>
  <verify>grep -E "(--tokenizers|split.*,|tokenizers.*verbose)" src/cli/index.ts returns expected CLI option and parsing logic</verify>
  <done>CLI accepts --tokenizers flag, parses comma-separated list, passes to processEpubs, defaults to ['gpt4']</done>
</task>

<task type="auto">
  <name>Extend JSON output for token_counts</name>
  <files>src/output/json.ts</files>
  <action>
    Extend src/output/json.ts to include token_counts and schema_version:

    **1. Import TokenizerResult type:**
    ```typescript
    import type { TokenizerResult } from '../tokenizers/types.js';
    ```

    **2. Extend EpubJsonResult interface:**
    ```typescript
    export interface EpubJsonResult {
      filename: string;
      file_path: string;
      title: string;
      author: string;
      word_count: number;
      language?: string;
      publisher?: string;
      // NEW: Token counts per tokenizer
      token_counts?: TokenizerResult[];  // Array of { name, count }
      // NEW: Schema version for output format
      schema_version?: string;
    }
    ```

    **3. Update generateJsonOutput to accept token_counts:**
    ```typescript
    export function generateJsonOutput(
      result: ProcessingResult,
      timestamp: string,
      tokenCounts?: Map<string, TokenizerResult[]>  // filename -> token_counts
    ): object {
      return {
        schema_version: '1.0',  // Per CONTEXT.md requirement
        generated_at: timestamp,
        summary: {
          total: result.total,
          successful: result.successful.length,
          failed: result.failed.length,
        },
        epubs: result.successful.map((epub) => {
          const epubResult: any = {
            filename: epub.filename,
            file_path: epub.file_path || epub.filename,
            title: epub.title,
            author: epub.author,
            word_count: epub.wordCount,
            language: epub.language,
            publisher: epub.publisher,
          };

          // Add token_counts if available
          if (tokenCounts && tokenCounts.has(epub.filename)) {
            epubResult.token_counts = tokenCounts.get(epub.filename);
          }

          return epubResult;
        }),
        failed: result.failed.map((f) => ({
          file: f.file,
          error: f.error,
          suggestion: f.suggestion,
        })),
      };
    }
    ```

    **4. Update writeJsonFile signature (temporarily accept optional param):**
    ```typescript
    export async function writeJsonFile(
      result: ProcessingResult,
      options: Partial<JsonOutputOptions> = {},
      tokenCounts?: Map<string, TokenizerResult[]>
    ): Promise<string> {
      // ... existing code ...
      const content = generateJsonOutput(result, new Date().toISOString(), tokenCounts);
      // ... rest of function ...
    }
    ```

    **JSON output structure (from CONTEXT.md):**
    - token_counts is array of objects: [{ name: "gpt4", count: 1234 }, { name: "claude", count: 1500 }]
    - schema_version is explicit string "1.0"
    - One file per EPUB (not one per tokenizer)

    Note: In this plan, we're extending the JSON structure but token_counts will be empty/undefined until plan 02-04 wires up the actual tokenization.

    Follow existing patterns:
    - Use Partial<> for options (like existing writeJsonFile)
    - Maintain backward compatibility (token_counts is optional)
    - Type-safe with Map for filename -> token_counts mapping
  </action>
  <verify>grep -E "(token_counts|schema_version|TokenizerResult)" src/output/json.ts returns expected extensions</verify>
  <done>EpubJsonResult has token_counts array and schema_version field, generateJsonOutput accepts tokenCounts parameter</done>
</task>

</tasks>

<verification>
After completion, verify:
- [ ] TypeScript compilation succeeds (npm run build)
- [ ] CLI has --tokenizers option with default 'gpt4'
- [ ] CLI parsing splits comma-separated list correctly
- [ ] JSON output includes schema_version: "1.0"
- [ ] JSON output structure includes token_counts field (even if undefined for now)
</verification>

<success_criteria>
1. epub-counter --help shows --tokenizers option
2. epub-counter --tokenizers gpt4,claude,hf:gpt2 book.epub parses tokenizers array correctly
3. JSON output has schema_version field
4. EpubJsonResult interface includes token_counts?: TokenizerResult[]
</success_criteria>

<output>
After completion, create `.planning/phases/02-tokenization-engine/02-03-SUMMARY.md`
</output>
