---
phase: 02-tokenization-engine
plan: 04
type: execute
wave: 4
depends_on: [02-02, 02-03]
files_modified:
  - src/errors/handler.ts
  - src/tokenizers/index.ts
  - src/cli/index.ts
autonomous: true
user_setup: []

must_haves:
  truths:
    - "EPUB processing pipeline tokenizes text with selected tokenizers"
    - "Tokenization results are accumulated per EPUB"
    - "JSON output contains populated token_counts array"
    - "Claude 3+ warning is displayed when Claude tokenizer is used"
    - "Large EPUBs are processed safely without memory issues"
  artifacts:
    - path: "src/tokenizers/index.ts"
      provides: "Tokenization orchestration module"
      exports: ["tokenizeText", "createTokenizers"]
      min_lines: 40
    - path: "src/errors/handler.ts"
      provides: "Updated processing with tokenization integration"
      contains: "tokenCounts"
    - path: "src/cli/index.ts"
      provides: "Updated CLI with memory management flag"
      contains: "--max-mb"
  key_links:
    - from: "src/errors/handler.ts"
      to: "src/tokenizers/index.ts"
      via: "import and call tokenizeText()"
      pattern: "tokenizeText.*text"
    - from: "src/errors/handler.ts"
      to: "src/output/json.ts"
      via: "pass tokenCounts to writeJsonFile()"
      pattern: "tokenCounts.*writeJsonFile"
    - from: "src/cli/index.ts"
      to: "src/errors/handler.ts"
      via: "pass tokenizers array to processEpubsWithErrors()"
      pattern: "tokenizers.*processEpubsWithErrors"
---

<objective>
Integrate tokenization into the EPUB processing pipeline, add memory management, and implement end-to-end tokenization with warnings.

Purpose: Wire the tokenizer implementations into the existing EPUB processing flow. The processing pipeline will now tokenize extracted text using the selected tokenizers, accumulate token counts per EPUB, and include them in JSON output. Also adds the --max-mb flag for memory management on large EPUBs and displays Claude 3+ inaccuracy warning.

Output: Functional tokenization pipeline integrated into processEpubsWithErrors(), with memory management and user warnings.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior plans
@.planning/phases/02-tokenization-engine/02-01-PLAN.md
@.planning/phases/02-tokenization-engine/02-02-PLAN.md
@.planning/phases/02-tokenization-engine/02-03-PLAN.md

# Codebase context
@src/errors/handler.ts
@src/cli/index.ts
@src/tokenizers/types.ts
@src/epub/text.ts
@src/output/json.ts

# Research findings
@.planning/phases/02-tokenization-engine/02-RESEARCH.md
@.planning/phases/02-tokenization-engine/02-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Create tokenization orchestration module</name>
  <files>src/tokenizers/index.ts</files>
  <action>
    Create src/tokenizers/index.ts to orchestrate tokenization:

    **1. Import tokenizer classes and types:**
    ```typescript
    import { GPT4Tokenizer } from './gpt.js';
    import { ClaudeTokenizer } from './claude.js';
    import { HFTokenizer } from './huggingface.js';
    import type { Tokenizer, TokenizerResult } from './types.js';
    ```

    **2. Create tokenizer instances from names:**
    ```typescript
    export function createTokenizers(names: string[]): Tokenizer[] {
      const tokenizers: Tokenizer[] = [];

      for (const name of names) {
        if (name === 'gpt4') {
          tokenizers.push(new GPT4Tokenizer());
        } else if (name === 'claude') {
          tokenizers.push(new ClaudeTokenizer());
        } else if (name.startsWith('hf:')) {
          // Extract model name from "hf:model-name"
          const modelName = name.replace(/^hf:/, '');
          tokenizers.push(new HFTokenizer(modelName));
        } else {
          // Unknown tokenizer - throw error
          throw new Error(`Unknown tokenizer: ${name}. Valid presets: gpt4, claude. Or use hf:model-name for Hugging Face models.`);
        }
      }

      return tokenizers;
    }
    ```

    **3. Orchestrate tokenization for multiple tokenizers:**
    ```typescript
    export async function tokenizeText(
      text: string,
      tokenizers: Tokenizer[]
    ): Promise<TokenizerResult[]> {
      const results: TokenizerResult[] = [];

      for (const tokenizer of tokenizers) {
        try {
          // Handle both sync and async countTokens
          const count = await Promise.resolve(tokenizer.countTokens(text));
          results.push({ name: tokenizer.name, count });
        } catch (error) {
          // Log error but continue with other tokenizers
          console.error(`Warning: Tokenizer '${tokenizer.name}' failed: ${error}`);
          results.push({ name: tokenizer.name, count: -1 });  // -1 indicates error
        }
      }

      return results;
    }
    ```

    **Key considerations:**
    - Factory function: createTokenizers() parses names and creates instances
    - Handles "gpt4", "claude", and "hf:model-name" formats
    - tokenizeText() handles both sync and async tokenizers (HF is async)
    - Error resilience: continue on tokenizer failure
    - Return -1 for failed tokenizers (distinguish from 0 tokens)

    Follow existing codebase patterns:
    - ES6 modules with .js extensions
    - JSDoc comments
    - Async/await for compatibility with async HF tokenizer
  </action>
  <verify>grep -E "(createTokenizers|tokenizeText|GPT4Tokenizer|ClaudeTokenizer|HFTokenizer)" src/tokenizers/index.ts returns expected exports</verify>
  <done>src/tokenizers/index.ts exports createTokenizers() and tokenizeText() functions</done>
</task>

<task type="auto">
  <name>Integrate tokenization into processing pipeline</name>
  <files>src/errors/handler.ts</files>
  <action>
    Update src/errors/handler.ts to integrate tokenization:

    **1. Import tokenization module:**
    ```typescript
    import { createTokenizers, tokenizeText } from '../tokenizers/index.js';
    import type { TokenizerResult } from '../tokenizers/types.js';
    ```

    **2. Update processEpubsWithErrors signature:**
    ```typescript
    export async function processEpubsWithErrors(
      filePaths: string[],
      verbose: boolean = false,
      outputDir: string = './results',
      tokenizerNames: string[] = ['gpt4'],  // NEW parameter
      maxMb: number = 500  // NEW parameter (from --max-mb flag)
    ): Promise<ProcessingResult & { tokenCounts: Map<string, TokenizerResult[]> }> {
      // ... implementation ...
    }
    ```

    **3. Create tokenizers once at the start:**
    ```typescript
    // Create tokenizer instances
    let tokenizers: any[] = [];
    try {
      tokenizers = createTokenizers(tokenizerNames);

      // Display Claude warning if needed
      if (tokenizerNames.includes('claude')) {
        console.warn('⚠️  Warning: Claude tokenizer is inaccurate for Claude 3+ models');
      }
    } catch (error) {
      console.error(`Error: Failed to create tokenizers: ${error}`);
      throw error;
    }
    ```

    **4. Tokenize after text extraction:**
    ```typescript
    // Inside the file processing loop
    // Extract text and count words (existing code)
    const text = extractText(parseResult.sections);
    const wordCount = countWords(text);

    // NEW: Tokenize with selected tokenizers
    let tokenResults: TokenizerResult[] = [];
    if (text.length > 0) {
      try {
        tokenResults = await tokenizeText(text, tokenizers);
      } catch (error) {
        console.error(`Warning: Tokenization failed for ${filename}: ${error}`);
      }
    }

    // Accumulate token counts by filename
    tokenCounts.set(filename, tokenResults);
    ```

    **5. Return tokenCounts in result:**
    ```typescript
    return {
      successful,
      failed,
      total: filePaths.length,
      tokenCounts,  // NEW: Map of filename -> TokenizerResult[]
    };
    ```

    **6. Add memory check (basic implementation):**
    ```typescript
    // Check text size before processing
    const maxBytes = maxMb * 1024 * 1024;  // Convert MB to bytes
    if (text.length > maxBytes) {
      throw new Error(`EPUB text size (${(text.length / 1024 / 1024).toFixed(2)}MB) exceeds --max-mb limit (${maxMb}MB)`);
    }
    ```

    **Key considerations from RESEARCH.md:**
    - Tokenize per EPUB (not per chapter) for Phase 2 - chapter-by-chapter is optimization for later
    - Memory limit: Check text size and throw error if exceeds --max-mb
    - Claude 3+ warning: Display when Claude tokenizer is in the list
    - Continue on error: Don't fail entire processing if tokenization fails

    **Note on memory management:**
    Per CONTEXT.md decision: Processing strategy is Claude's discretion. For v1, we're implementing simple size check. File-by-file (chapter-level) processing can be added in Phase 3 if needed based on performance testing.

    Follow existing patterns:
    - Continue-on-error pattern already in handler.ts
    - Verbose logging for tokenization results
    - Map for accumulating results (same pattern as could be used for other aggregations)
  </action>
  <verify>grep -E "(createTokenizers|tokenizeText|tokenCounts|maxMb)" src/errors/handler.ts returns expected integration</verify>
  <done>processEpubsWithErrors accepts tokenizerNames and maxMb, calls tokenizeText(), returns tokenCounts, displays Claude warning</done>
</task>

<task type="auto">
  <name>Add --max-mb flag and wire up CLI</name>
  <files>src/cli/index.ts</files>
  <action>
    Update src/cli/index.ts to add --max-mb flag and wire tokenization:

    **1. Add --max-mb option:**
    ```typescript
    program
      // ... existing options ...
      .option('--max-mb <size>', 'Maximum EPUB text size in MB (default: 500)', '500')
      .action((paths, options) => {
        // Parse maxMb as number
        const maxMb = parseInt(options.maxMb || '500', 10);

        // ... rest of action handler ...
      });
    ```

    **2. Update processEpubs call:**
    ```typescript
    async function processEpubs(inputPaths: string[], options: any): Promise<void> {
      // ... existing code ...

      // Parse tokenizers and maxMb
      const tokenizers = options.tokenizers ? options.tokenizers.split(',').map((t: string) => t.trim()) : ['gpt4'];
      const maxMb = parseInt(options.maxMb || '500', 10);

      // Pass to processEpubsWithErrors
      const result = await processEpubsWithErrors(
        allFiles,
        options.verbose,
        outputDir,
        tokenizers,  // NEW
        maxMb         // NEW
      );

      // ... display results ...

      // Pass tokenCounts to JSON writer
      const jsonPath = await writeJsonFile(result, { outputDir }, result.tokenCounts);

      // ... rest of function ...
    }
    ```

    **3. Update writeJsonFile call:**
    ```typescript
    // Generate and write JSON results file with token counts
    const jsonPath = await writeJsonFile(result, { outputDir }, result.tokenCounts);
    ```

    **Key considerations:**
    - --max-mb defaults to 500MB per CONTEXT.md
    - Parse as integer, validate it's a positive number
    - Pass tokenizers and maxMb to processEpubsWithErrors
    - Pass result.tokenCounts to writeJsonFile (from plan 02-03)

    Follow existing CLI patterns:
    - Default value in .option() call
    - Integer parsing with validation
    - Pass through to processing function
  </action>
  <verify>grep -E "(--max-mb|maxMb|result\.tokenCounts)" src/cli/index.ts returns expected flag and wiring</verify>
  <done>CLI has --max-mb flag (default 500), passes tokenizers and maxMb to processEpubsWithErrors, passes tokenCounts to writeJsonFile</done>
</task>

</tasks>

<verification>
After completion, verify:
- [ ] TypeScript compilation succeeds (npm run build)
- [ ] CLI accepts --tokenizers and --max-mb flags
- [ ] processEpubsWithErrors integrates tokenizeText()
- [ ] JSON output contains populated token_counts array
- [ ] Claude 3+ warning displays when 'claude' is in tokenizers list
- [ ] Error is thrown when EPUB text exceeds --max-mb limit
</verification>

<success_criteria>
1. Run: epub-counter --tokenizers gpt4,claude book.epub produces JSON with token_counts for both tokenizers
2. Claude warning displays: "⚠️ Warning: Claude tokenizer is inaccurate for Claude 3+ models"
3. --max-mb flag limits processing (test with small value like 1MB)
4. Token counts are accurate (compare with known samples)
</success_criteria>

<output>
After completion, create `.planning/phases/02-tokenization-engine/02-04-SUMMARY.md`
</output>
